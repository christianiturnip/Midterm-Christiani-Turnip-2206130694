{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnXqpIqawwrMbCWoQNc446",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christianiturnip/Midterm-Christiani-Turnip-2206130694/blob/main/Mid_Term_ADL_Christiani_Turnip_2206130694.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMpK8kDR0v76"
      },
      "outputs": [],
      "source": [
        "# Install the required libraries\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install opencv-python\n",
        "!pip install -U git+https://github.com/ultralytics/yolov5.git\n",
        "\n",
        "# Set GPU as the runtime\n",
        "import torch\n",
        "\n",
        "# Check if GPU is available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using {device} device')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "r2NriJJb0453"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "\n",
        "# Load the YOLOv5 model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)  # Load a pre-trained YOLOv5 model\n",
        "\n",
        "# Define the video path\n",
        "video_path = '/content/drive/MyDrive/vidioadl.mp4'  # Replace with your video path"
      ],
      "metadata": {
        "id": "Hs1HZU6l09NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "# Load the YOLOv5 model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)  # Load a pre-trained YOLOv5 model\n",
        "\n",
        "# Define the video path\n",
        "video_path = '/content/drive/MyDrive/vidioadl.mp4'  # Replace with your video path\n",
        "\n",
        "# Open the video file\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Process each frame\n",
        "frame_count = 0\n",
        "max_frames = 100  # Set a limit for frames to process\n",
        "\n",
        "while cap.isOpened() and frame_count < max_frames:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Perform inference on the frame\n",
        "    results = model(frame)\n",
        "\n",
        "    # Extract bounding boxes and class labels\n",
        "    bbox_xyxy = results.xyxy[0]  # Bounding boxes\n",
        "    for *bbox, conf, cls in bbox_xyxy:  # bbox format: [x1, y1, x2, y2, confidence, class]\n",
        "        x1, y1, x2, y2 = map(int, bbox)  # Convert to integers\n",
        "        label = model.names[int(cls)]  # Get class label\n",
        "\n",
        "        # Draw bounding box on the frame\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "        cv2.putText(frame, f'{label} {conf:.2f}', (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "\n",
        "    # Display the frame using cv2_imshow\n",
        "    cv2_imshow(frame)\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "# Release the video capture object\n",
        "cap.release()"
      ],
      "metadata": {
        "id": "P5wo28Vy0_4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store detection results\n",
        "detections = []\n",
        "\n",
        "# Process each frame\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    results = model(frame)\n",
        "    bbox_xyxy = results.xyxy[0]\n",
        "\n",
        "    # Preprocess the results\n",
        "    for *bbox, conf, cls in bbox_xyxy:\n",
        "        x1, y1, x2, y2 = map(int, bbox)\n",
        "        label = model.names[int(cls)]\n",
        "        detection = {\n",
        "            'frame': frame_number,  # frame number\n",
        "            'bounding_box': [x1, y1, x2, y2],\n",
        "            'confidence': conf.item(),\n",
        "            'class_label': label\n",
        "        }\n",
        "        detections.append(detection)\n",
        "\n",
        "    frame_number += 1\n",
        "\n",
        "# Save the detections for analysis\n",
        "import pandas as pd\n",
        "detections_df = pd.DataFrame(detections)\n",
        "detections_df.to_csv('/content/drive/MyDrive/detections.csv', index=False)"
      ],
      "metadata": {
        "id": "d4cjKjbI1Dtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple tracking logic (not using a dedicated tracking library)\n",
        "tracked_objects = {}\n",
        "\n",
        "# Iterate through the detections\n",
        "for detection in detections:\n",
        "    frame_number = detection['frame']\n",
        "    bbox = detection['bounding_box']\n",
        "    label = detection['class_label']\n",
        "\n",
        "    # Maintain object identity\n",
        "    if label not in tracked_objects:\n",
        "        tracked_objects[label] = []\n",
        "\n",
        "    tracked_objects[label].append({\n",
        "        'frame': frame_number,\n",
        "        'bounding_box': bbox\n",
        "    })\n",
        "\n",
        "# Example of accessing tracking data\n",
        "for obj, tracks in tracked_objects.items():\n",
        "    print(f'Tracking {obj}: {len(tracks)} frames')"
      ],
      "metadata": {
        "id": "zsdmx1j91GRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True, force_reload=True)\n",
        "model.eval()  # Set to evaluation mode"
      ],
      "metadata": {
        "id": "ArnBUYa-1JAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frame_skip = 2  # Process every 2nd frame\n",
        "frame_number = 0\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    if frame_number % frame_skip == 0:\n",
        "        results = model(frame)\n",
        "        # Processing results...\n",
        "    frame_number += 1"
      ],
      "metadata": {
        "id": "Th3XKEsA1LbP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}